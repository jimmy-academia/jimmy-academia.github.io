<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Chin-Yuan Yeh</title>
  
  <meta name="author" content="Chin-Yuan Yeh">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <!-- <link rel="icon" type="image/png" href="images/prof_pic.jpg"> -->
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Chin-Yuan Yeh</name>
              </p>
              <p>I am a fourth-year PhD student at National Taiwan University. My research focus on deepfake, adversarial attack, and data mining.
              </p>
              <p>
              <!-- At NTU I've worked on white-box and black-box adversarial attack against image-to-image translation GANs, with potential application on tackling the problem of DeepFake/DeepNude. -->
              </p>
              <p style="text-align:center">
                <a href="mailto:marrch30@gmail.com">Email</a> &nbsp/&nbsp
                <a href="data/Chin_Yuan_Yeh_Curriculum_Vitae.pdf">CV</a> &nbsp/&nbsp
                <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp/&nbsp -->
                <a href="https://scholar.google.com/citations?user=O54-RPoAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/jimmy_cyyeh">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/jimmy-academia">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/mypic.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/mypic.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <heading>Publications</heading>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>
                Disrupting Image-Translation-Based DeepFake Algorithms with Adversarial Attacks
                </papertitle>
                <br>
                <strong>Chin-Yuan Yeh</strong>, Hsi-Wen Chen, Shang-Lun Tsai, Sheng-De Wang
                <br>
                <em>WACV Deepfakes and Presentation Attacks in Biometrics workshop</em>, 2020 
                <br>
                <a href="https://github.com/jimmy-academia/Adversarial-Attack-CycleGAN-and-pix2pix">source code</a>
                <p></p>
                <p>I am the first to introduce adversarial attack strategies to incapacitate Deepfake models, i.e., image translation GANs (CycleGAN, pix2pix, and pix2pixHD). I develop two attacks: Nullifying Attack which minimizes Deepfake modification, and Distorting Attack which causes distortion to deepfake outputs. Includes case studies on repeated inference, ensemble attack. </p>
              </td>
            </tr>
          <tr>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Attack As the Best Defense: Nullifying Image-to-Image Translation GANs via Limit-Aware Adversarial Attack
              </papertitle>
              <br>
              <strong>Chin-Yuan Yeh</strong>, Hsi-Wen Chen, Hong-Han Shuai, De-Nian Yang, Ming-Syan Chen
              <br>
              <em>ICCV</em> 2021
              <br>
              <a href="https://arxiv.org/abs/2110.02516">arXiv</a>
              /
              <a href="https://www.youtube.com/watch?v=GtmDkHQqA6k&t=9s&ab_channel=JimmyYeh">video</a>
              /
              <a href="https://github.com/jimmy-academia/LaSGSA">source code</a>
              <p></p>
              <p>I develop Limit-Aware Self-Guiding Gradient Sliding Attack (LaSGSA), a query-based black-box norm-bounded adversarial attack against Img2Img GANs (potentially used as deepfakes) with three optimization acceleration techniques: limit-aware RGF which restricts query sampling within the ε-bound, gradient sliding mechanism that propagates after being clipped by the ε-bound, and self-guiding prior, which leverages the semantic consistency of Img2Img GANs causing the Jacobian matrix of its mapping to be diagonal.</p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Planning Data Poisoning Attacks on Heterogeneous Recommender Systems in a Multiplayer Setting
                </papertitle>
                <br>
                <strong>Chin-Yuan Yeh</strong>, Hsi-Wen Chen, De-Nian Yang, Wang-Chien Lee, Philip S. Yu, Ming-Syan Chen
                <br>
                <em>ICDE</em> 2023
                <br>
                <a href="https://github.com/jimmy-academia/MSOPDS">source code</a>
                <p></p>
                <p>I develop Multilevel Stackelberg Optimization over Progressive Differentiable Surrogate (MSOPDS), a data poisoning technique against Heterogeneous RecSys, addressing the scenario of multiple attackers poisoning the same Recommendation System, where the first attacker aims to prevent subsequent attackers from harming his poisoning objective. MSOPDS leverages Stackelberg Game analysis between the first attacker the subsequent attackers’ actions and projection techniques to navigate the gradient descent over discrete RecSys operations.</p>
              </td>
            </tr>
            <tr>
              <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle>Does Audio Deepfake Detection Rely on Artifacts?
                  </papertitle>
                  <br>
                  Tsu-Hsien Shih, <strong>Chin-Yuan Yeh</strong>, Ming-Syan Chen
                  <br>
                  <em>ICASSP</em> 2024
                  <p></p>
                  <p>I develop Multilevel Stackelberg Optimization over Progressive Differentiable Surrogate (MSOPDS), a data poisoning technique against Heterogeneous RecSys, addressing the scenario of multiple attackers poisoning the same Recommendation System, where the first attacker aims to prevent subsequent attackers from harming his poisoning objective. MSOPDS leverages Stackelberg Game analysis between the first attacker the subsequent attackers’ actions and projection techniques to navigate the gradient descent over discrete RecSys operations.</p>
                </td>
              </tr>


        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
